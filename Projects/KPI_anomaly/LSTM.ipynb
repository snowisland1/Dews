{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from numpy import arange, sin, pi, random\n",
    "\n",
    "# Global hyper-parameters\n",
    "sequence_length = 1940\n",
    "random_data_dup = 5  # each sample randomly duplicated between 0 and 9 times, see dropin function\n",
    "epochs = 1\n",
    "batch_size = 50\n",
    "\n",
    "# consider delay threshold and missing segments\n",
    "def get_range_proba(predict, label, delay=7):\n",
    "\n",
    "    splits = np.where(label[1:] != label[:-1])[0] + 1\n",
    "    is_anomaly = label[0] == 1\n",
    "    new_predict = np.array(predict)\n",
    "    pos = 0\n",
    "\n",
    "    for sp in splits:\n",
    "        if is_anomaly:\n",
    "            if 1 in predict[pos:min(pos + delay + 1,sp)]:\n",
    "                new_predict[pos: sp] = 1\n",
    "            else:\n",
    "                new_predict[pos: sp] = 0\n",
    "        is_anomaly = not is_anomaly\n",
    "        pos = sp\n",
    "    sp = len(label)\n",
    "\n",
    "    if is_anomaly:  #anomaly in the end\n",
    "        if 1 in predict[pos: min(pos + delay+1,sp)]:\n",
    "            new_predict[pos: sp] = 1\n",
    "        else:\n",
    "            new_predict[pos: sp] = 0\n",
    "    return new_predict\n",
    "\n",
    "# set missing = 0\n",
    "def reconstruct_label(timestamp, label):\n",
    "    timestamp = np.asarray(timestamp, np.int64)\n",
    "    timestamp_sorted = np.asarray(timestamp[np.argsort(timestamp)])\n",
    "    interval = np.min(np.diff(timestamp_sorted))\n",
    "    if interval == 0:\n",
    "        print(timestamp_sorted)\n",
    "    idx = (timestamp_sorted - timestamp_sorted[0]) // interval\n",
    "    new_label = np.zeros(shape=((timestamp_sorted[-1] - timestamp_sorted[0]) // interval + 1,), dtype=np.int)\n",
    "    new_label[idx] = label\n",
    "    return new_label\n",
    "\n",
    "def label_evaluation(truth_df, result_df, delay=7):\n",
    "    data = {'result': False, 'data': \"\", 'message': \"\"}\n",
    "    kpi_names = truth_df['KPI ID'].values\n",
    "    kpi_names = np.unique(kpi_names)\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    for kpi_name in kpi_names:\n",
    "        truth = truth_df[truth_df[\"KPI ID\"] == kpi_name]\n",
    "        y_true = reconstruct_label(truth[\"timestamp\"], truth[\"label\"])\n",
    "        try:\n",
    "            result = result_df[result_df[\"KPI ID\"] == kpi_name]\n",
    "            y_pred = reconstruct_label(result[\"timestamp\"], result[\"predict\"])\n",
    "        except:\n",
    "            data['message'] = \"The file you submitted need contain 'predict','timestamp' and  \\\n",
    "                             'KPI ID' columns\"\n",
    "            return json.dumps(data)\n",
    "        try:\n",
    "            assert np.array_equal(len(y_true),len(y_pred)) == True\n",
    "        except:\n",
    "            data['message'] = \"The length of your submitted file is wrong\"\n",
    "            return json.dumps(data)\n",
    "\n",
    "        y_pred = get_range_proba(y_pred, y_true, delay)\n",
    "        y_true_list.append(y_true)\n",
    "        y_pred_list.append(y_pred)\n",
    "\n",
    "    fscore = f1_score(np.concatenate(y_true_list), np.concatenate(y_pred_list))\n",
    "    data['result'] = True\n",
    "    data['data'] = fscore\n",
    "    data['message'] = 'success'\n",
    "    print(json.dumps(data))\n",
    "    return json.dumps(data)\n",
    "\n",
    "def dropin(X, y):\n",
    "    \"\"\" The name suggests the inverse of dropout, i.e. adding more samples. See Data Augmentation section at\n",
    "    http://simaaron.github.io/Estimating-rainfall-from-weather-radar-readings-using-recurrent-neural-networks/\n",
    "    :param X: Each row is a training sequence\n",
    "    :param y: Tne target we train and will later predict\n",
    "    :return: new augmented X, y\n",
    "    \"\"\"\n",
    "    print(\"X shape:\", X.shape)\n",
    "    print(\"y shape:\", y.shape)\n",
    "    X_hat = []\n",
    "    y_hat = []\n",
    "    for i in range(0, len(X)):\n",
    "        for j in range(0, np.random.random_integers(0, random_data_dup)):\n",
    "            X_hat.append(X[i, :])\n",
    "            y_hat.append(y[i, ])\n",
    "    return np.asarray(X_hat), np.asarray(y_hat)\n",
    "\n",
    "def make_samples(samples):\n",
    "    data = samples.value\n",
    "    print(\"Length of Data\", len(data))\n",
    "    # train data\n",
    "    print(\"Creating train data...\")\n",
    "    result = []\n",
    "    for index in range(0, len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    result = np.array(result)  # shape (samples, sequence_length)    \n",
    "    print(\"Train data shape  : \", result.shape)\n",
    "    np.random.shuffle(result)  # shuffles in-place\n",
    "    X_train = result[:, :-1]\n",
    "    y_train = result[:, -1]\n",
    "    X_train,y_train = dropin(X_train,y_train)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "    return X_train,y_train\n",
    "\n",
    "def make_submit_samples(samples):\n",
    "    data = samples.value\n",
    "    print(\"Length of Data\", len(data))\n",
    "    # train data\n",
    "    print(\"Creating train data...\")\n",
    "    result = []\n",
    "    for index in range(0, len(data) - 1939):\n",
    "        result.append(data[index: index+1939])\n",
    "    result = np.array(result)  # shape (samples, sequence_length)\n",
    "    print(\"Train data shape  : \", result.shape)\n",
    "    np.random.shuffle(result)  # shuffles in-place\n",
    "    X_train = result\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0],X_train.shape[1],1))\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    layers = {'input': 1, 'hidden1': 64, 'hidden2': 256, 'hidden3': 100, 'output': 1}\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        input_length=sequence_length - 1,\n",
    "        input_dim=1,\n",
    "        #input_shape = (1,1440),\n",
    "        output_dim=layers['hidden1'],\n",
    "        return_sequences=True))\n",
    "    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(\n",
    "        layers['hidden2'],\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(LSTM(\n",
    "        layers['hidden3'],\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(\n",
    "            output_dim=layers['output']))\n",
    "    model.add(Activation(\"linear\"))\n",
    "\n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def run_network(model=None, data=None):\n",
    "    global_start_time = time.time()\n",
    "    #X_train,y_train = make_samples(samples_train)\n",
    "    #X_testt,y_testt = make_samples(samples_testt)\n",
    "    print('\\nData Loaded. Compiling...\\n')\n",
    "    if model is None:\n",
    "        model = build_model()\n",
    "\n",
    "    try:\n",
    "        print(\"Training...\")\n",
    "        model.fit(\n",
    "                X_train, y_train,\n",
    "                batch_size=batch_size, nb_epoch=epochs, validation_split=0.05)\n",
    "        print(\"Predicting...\")\n",
    "        predicted = model.predict(X_testt)\n",
    "        print(\"Reshaping predicted\")\n",
    "        predicted = np.reshape(predicted, (predicted.size,))\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"prediction exception\")\n",
    "        print('Training duration (s) : ', time.time() - global_start_time)\n",
    "    return model, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data 131795\n",
      "Creating train data...\n",
      "Train data shape  :  (129855, 1940)\n",
      "X shape: (129855, 1939)\n",
      "y shape: (129855,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sheldon/anaconda2/envs/dnn/lib/python3.6/site-packages/ipykernel_launcher.py:98: DeprecationWarning: This function is deprecated. Please call randint(0, 5 + 1) instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Data 131795\n",
      "Creating train data...\n",
      "Train data shape  :  (129856, 1939)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((324412, 1939, 1), (324412,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_train=pd.read_csv(\"./samples/samples_train.csv\")\n",
    "samples_testt=pd.read_csv(\"./samples/samples_test.csv\")\n",
    "X_train,y_train = make_samples(samples_train)\n",
    "X_testt = make_submit_samples(samples_testt)\n",
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Loaded. Compiling...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sheldon/anaconda2/envs/dnn/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/sheldon/anaconda2/envs/dnn/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(1939, 1), units=64)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/sheldon/anaconda2/envs/dnn/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=1)`\n",
      "/Users/sheldon/anaconda2/envs/dnn/lib/python3.6/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Time :  0.06903505325317383\n",
      "Training...\n",
      "Train on 308191 samples, validate on 16221 samples\n",
      "Epoch 1/1\n",
      "   500/308191 [..............................] - ETA: 47:54:54 - loss: 1.7640"
     ]
    }
   ],
   "source": [
    "model, predicted = run_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(X_testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.897982  ,  1.89176536,  1.88228869, ...,  1.87918818,\n",
       "          1.8881855 ,  1.86921906]],\n",
       "\n",
       "       [[ 1.897982  ,  1.89176536,  1.88228869, ...,  1.87918818,\n",
       "          1.8881855 ,  1.86921906]],\n",
       "\n",
       "       [[ 1.897982  ,  1.89176536,  1.88228869, ...,  1.87918818,\n",
       "          1.8881855 ,  1.86921906]],\n",
       "\n",
       "       ..., \n",
       "       [[ 1.897982  ,  1.89176536,  1.88228869, ...,  1.87918818,\n",
       "          1.8881855 ,  1.86921906]],\n",
       "\n",
       "       [[ 1.897982  ,  1.89176536,  1.88228869, ...,  1.87918818,\n",
       "          1.8881855 ,  1.86921906]],\n",
       "\n",
       "       [[ 1.89798188,  1.89176524,  1.88228869, ...,  1.87918806,\n",
       "          1.8881855 ,  1.86921906]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted[19][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
